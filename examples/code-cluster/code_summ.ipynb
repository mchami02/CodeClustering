{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import PLBartForConditionalGeneration, PLBartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the code snippets\n",
    "code_clusters = pd.read_csv(\"public/code_clusters_9999.csv\")\n",
    "code_snippets = code_clusters['Code_Block'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mchami/ETH/AIResearch/SigmaCluster/.env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/mchami/ETH/AIResearch/SigmaCluster/.env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/mchami/ETH/AIResearch/SigmaCluster/.env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.98` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "['Returns 1 or 2 depending on the value of n. If n is not a number this function will return 0 or 1.']\n"
     ]
    }
   ],
   "source": [
    "### Create the model\n",
    "tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-python-en_XX\", src_lang=\"python\", tgt_lang=\"en_XX\")\n",
    "code_snippet = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\"\"\"\n",
    "inputs = tokenizer(code_snippet, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "# print(\"inputs\", inputs)\n",
    "model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-python-en_XX\")\n",
    "translated_tokens = model.generate(\n",
    "    **inputs,\n",
    "    decoder_start_token_id=tokenizer.lang_code_to_id[\"__en_XX__\"],\n",
    "    max_length=50,            # Maximum length of the generated summary\n",
    "    min_length=10,            # Minimum length of the generated summary\n",
    "    num_beams=40,              # Number of beams for beam search\n",
    "    no_repeat_ngram_size=2,   # Prevent repeating 2-grams\n",
    "    early_stopping=False,      # Stop early when num_beams sentences are finished\n",
    "    length_penalty=1.0,       # Length penalty\n",
    "    temperature=0.7,          # Temperature for next token sampling\n",
    "    top_k=50,                 # Top-k filtering\n",
    "    top_p=0.98,               # Nucleus filtering\n",
    "    repetition_penalty=1.2    # Penalty for repetition\n",
    ")\n",
    "summary = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "print(code_snippet)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: if 0: elif n == 1: return 1 else: return fibonacci(n-1) + fibonacci(n-2)\n"
     ]
    }
   ],
   "source": [
    "from transformers import PLBartForConditionalGeneration, PLBartTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"uclanlp/plbart-base\"\n",
    "tokenizer = PLBartTokenizer.from_pretrained(model_name)\n",
    "model = PLBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Define the code snippet to summarize\n",
    "code_snippet = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\"\"\"\n",
    "\n",
    "# Prepend the code with a task-specific prefix indicating the language and task\n",
    "input_text = f\"<python> summarize: {code_snippet}\"\n",
    "\n",
    "# Tokenize the input code snippet\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(inputs.input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the summary\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'trl-internal-testing/tiny-random-PLBartForConditionalGeneration'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'trl-internal-testing/tiny-random-PLBartForConditionalGeneration' is the correct path to a directory containing all relevant files for a PLBartTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/mchami/ETH/AIResearch/SigmaCluster/examples/code-cluster/code_summ.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mchami/ETH/AIResearch/SigmaCluster/examples/code-cluster/code_summ.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load model directly\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mchami/ETH/AIResearch/SigmaCluster/examples/code-cluster/code_summ.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mchami/ETH/AIResearch/SigmaCluster/examples/code-cluster/code_summ.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m PLBartTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mtrl-internal-testing/tiny-random-PLBartForConditionalGeneration\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mchami/ETH/AIResearch/SigmaCluster/examples/code-cluster/code_summ.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mtrl-internal-testing/tiny-random-PLBartForConditionalGeneration\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mchami/ETH/AIResearch/SigmaCluster/examples/code-cluster/code_summ.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Define the code snippet to summarize\u001b[39;00m\n",
      "File \u001b[0;32m~/ETH/AIResearch/SigmaCluster/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2073\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2067\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   2068\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2069\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2070\u001b[0m     )\n\u001b[1;32m   2072\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 2073\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2074\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2075\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2076\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2077\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2078\u001b[0m     )\n\u001b[1;32m   2080\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   2081\u001b[0m     \u001b[39mif\u001b[39;00m file_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'trl-internal-testing/tiny-random-PLBartForConditionalGeneration'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'trl-internal-testing/tiny-random-PLBartForConditionalGeneration' is the correct path to a directory containing all relevant files for a PLBartTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = PLBartTokenizer.from_pretrained(\"trl-internal-testing/tiny-random-PLBartForConditionalGeneration\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"trl-internal-testing/tiny-random-PLBartForConditionalGeneration\")\n",
    "\n",
    "# Define the code snippet to summarize\n",
    "code_snippet = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\"\"\"\n",
    "\n",
    "# Prepend the code with a task-specific prefix indicating the language and task\n",
    "input_text = f\"<python> summarize: {code_snippet}\"\n",
    "\n",
    "# Tokenize the input code snippet\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(inputs.input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the summary\n",
    "print(\"Summary:\", summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mchami/ETH/AIResearch/SigmaCluster/.env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### model choice\n",
    "from transformers import PLBartTokenizer, PLBartForConditionalGeneration\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-python-en_XX\", src_lang=\"python\", tgt_lang=\"en_XX\")\n",
    "model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-python-en_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(input):\n",
    "    # Tokenize the input code snippet\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate the summary with hyperparameters\n",
    "    translated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        decoder_start_token_id=tokenizer.lang_code_to_id[\"__en_XX__\"],\n",
    "        max_length=50,            # Maximum length of the generated summary\n",
    "        min_length=30,            # Minimum length of the generated summary\n",
    "        num_beams=20,             # Number of beams for beam search\n",
    "        no_repeat_ngram_size=2,   # Prevent repeating 2-grams\n",
    "        early_stopping=False,     # Stop early when num_beams sentences are finished\n",
    "        length_penalty=0.5,       # Length penalty\n",
    "        temperature=0.7,          # Temperature for next token sampling\n",
    "        top_k=50,                 # Top-k filtering\n",
    "        top_p=0.95,               # Nucleus filtering\n",
    "        repetition_penalty=1.2    # Penalty for repetition\n",
    "    )\n",
    "\n",
    "    # Decode and print the summary\n",
    "    summary = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mchami/ETH/AIResearch/SigmaCluster/.env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/mchami/ETH/AIResearch/SigmaCluster/.env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the code snippet\n",
    "summaries = []\n",
    "for code_snippet in code_snippets[:10]:\n",
    "    # Decode and print the summary\n",
    "    summary = generate_summary(translated_tokens)\n",
    "    summaries.append(summary)\n",
    "    \n",
    "df = pd.DataFrame({\"Code_Block\": code_snippets[:10], \"Summary\": summaries}, columns=[\"Code_Block\", \"Summary\"])\n",
    "df.to_csv(\"public/code_summaries.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snippet</th>\n",
       "      <th>rewritten_intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>os.kill(os.getpid(), signal.SIGUSR1)</td>\n",
       "      <td>send a signal `signal.SIGUSR1` to the current ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bytes.fromhex('4a4b4c').decode('utf-8')</td>\n",
       "      <td>decode a hex string '4a4b4c' to UTF-8.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all(x == myList[0] for x in myList)</td>\n",
       "      <td>check if all elements in list `myList` are ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>print('%*s : %*s' % (20, 'Python', 20, 'Very G...</td>\n",
       "      <td>format number of spaces between strings `Pytho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d.decode('cp1251').encode('utf8')</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>re.findall('http://[^t][^s\"]+\\\\.html', document)</td>\n",
       "      <td>match urls whose domain doesn't start with `t`...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>mystring.replace(' ', '! !').split('!')</td>\n",
       "      <td>split a string `mystring` considering the spac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>open(path, 'r')</td>\n",
       "      <td>open file `path` with mode 'r'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>[[sum(item) for item in zip(*items)] for items...</td>\n",
       "      <td>sum elements at the same index in list `data`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>a[:, (np.newaxis)]</td>\n",
       "      <td>add a new axis to array `a`</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               snippet  \\\n",
       "0                 os.kill(os.getpid(), signal.SIGUSR1)   \n",
       "1              bytes.fromhex('4a4b4c').decode('utf-8')   \n",
       "2                  all(x == myList[0] for x in myList)   \n",
       "3    print('%*s : %*s' % (20, 'Python', 20, 'Very G...   \n",
       "4                    d.decode('cp1251').encode('utf8')   \n",
       "..                                                 ...   \n",
       "495   re.findall('http://[^t][^s\"]+\\\\.html', document)   \n",
       "496            mystring.replace(' ', '! !').split('!')   \n",
       "497                                    open(path, 'r')   \n",
       "498  [[sum(item) for item in zip(*items)] for items...   \n",
       "499                                 a[:, (np.newaxis)]   \n",
       "\n",
       "                                      rewritten_intent  \n",
       "0    send a signal `signal.SIGUSR1` to the current ...  \n",
       "1               decode a hex string '4a4b4c' to UTF-8.  \n",
       "2    check if all elements in list `myList` are ide...  \n",
       "3    format number of spaces between strings `Pytho...  \n",
       "4                                                 None  \n",
       "..                                                 ...  \n",
       "495  match urls whose domain doesn't start with `t`...  \n",
       "496  split a string `mystring` considering the spac...  \n",
       "497                     open file `path` with mode 'r'  \n",
       "498      sum elements at the same index in list `data`  \n",
       "499                        add a new axis to array `a`  \n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read json file\n",
    "import json\n",
    "\n",
    "with open('public/conala-paired-test.json') as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "\n",
    "eval_df = pd.DataFrame(data)[['snippet', 'rewritten_intent']]\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_snippets = eval_df['snippet'].to_numpy()[:100]\n",
    "eval_intents = eval_df['rewritten_intent'].to_numpy()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mchami/ETH/AIResearch/SigmaCluster/.env/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:1699: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "/Users/mchami/ETH/AIResearch/SigmaCluster/.env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 50, but your input_length is only 17. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.kill(os.getpid(), signal.SIGUSR1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'return 0 }\"'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\n",
    "\n",
    "pipeline = SummarizationPipeline(\n",
    "    model=AutoModelWithLMHead.from_pretrained(\"SEBIS/code_trans_t5_base_code_documentation_generation_php_multitask\"),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"SEBIS/code_trans_t5_base_code_documentation_generation_php_multitask\", skip_special_tokens=True),\n",
    "    device=0\n",
    ")\n",
    "tokenized_code = eval_snippets[0]\n",
    "pipeline([tokenized_code], max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_intents = []\n",
    "for eval_snippet in eval_snippets:\n",
    "    summary = generate_summary(eval_snippet)\n",
    "    generated_intents.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 Score: 2.2437748935664263\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(generated_intents, [[eval_intents[i]] for i in range(len(eval_intents))], smooth_method='exp')\n",
    "print(f\"BLEU-4 Score: {bleu.score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
